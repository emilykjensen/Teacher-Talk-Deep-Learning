{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using LSTM network for sentiment classification on IMDB data\n",
    "\n",
    "Source: https://keras.io/examples/imdb_lstm/\n",
    "\n",
    "It looks like the architecture here will be similar to our project, but the dimenisons of the input and output and data will be different.\n",
    "\n",
    "About the data:\n",
    "\n",
    "\"Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer '3' encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: 'only consider the top 10,000 most common words, but eliminate the top 20 most common words'.\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How each sentence is represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   15   256     4     2     7  3766     5   723    36    71    43   530\n",
      "   476    26   400   317    46     7     4 12118  1029    13   104    88\n",
      "     4   381    15   297    98    32  2071    56    26   141     6   194\n",
      "  7486    18     4   226    22    21   134   476    26   480     5   144\n",
      "    30  5535    18    51    36    28   224    92    25   104     4   226\n",
      "    65    16    38  1334    88    12    16   283     5    16  4472   113\n",
      "   103    32    15    16  5345    19   178    32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses the embedding layer to learn a word embedding within the model. This takes the vocabulary size and desired embedding dimension as arguments. Next an LSTM layer with dropout is added. The final dense layer outputs the classification, with dimension 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes about 30 minutes to train. The example originally trained for 15 epochs, but I scaled that back to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chloe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 371s 15ms/step - loss: 0.4543 - accuracy: 0.7877 - val_loss: 0.3607 - val_accuracy: 0.8400\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 361s 14ms/step - loss: 0.2982 - accuracy: 0.8820 - val_loss: 0.3726 - val_accuracy: 0.8380\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 353s 14ms/step - loss: 0.2179 - accuracy: 0.9159 - val_loss: 0.4520 - val_accuracy: 0.8122\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 429s 17ms/step - loss: 0.1577 - accuracy: 0.9412 - val_loss: 0.4608 - val_accuracy: 0.8308\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 394s 16ms/step - loss: 0.1107 - accuracy: 0.9608 - val_loss: 0.5363 - val_accuracy: 0.8200\n",
      "25000/25000 [==============================] - 83s 3ms/step\n",
      "Test score: 0.5362590254974365\n",
      "Test accuracy: 0.8199999928474426\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
